#  Copyright 2023 Google LLC
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#       https://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.


# This script, forward_pass_logits_checker.py, verifies the correctness of logits generated by the MaxText or HuggingFace
# implementation of a model by comparing them against a reference "golden" logits file for a set of input prompts.
#
# It supports multiple models and expects a reference file in JSON Lines (.jsonl) format, specified via the
# --golden_logits_path argument. If not provided, it defaults to:
#   MaxText/test_assets/golden_data_<model_name>.jsonl
# For example:
#   MaxText/test_assets/golden_data_llama2-7b.jsonl
#
# Each line in the golden .jsonl file should be a dictionary with the following keys:
#   1. prompt: a string prompt, e.g., "I love to"
#   2. tokens: the token IDs resulting from tokenizing the prompt
#   3. logits: the expected (golden) logits output by the model for the given prompt
#
# The script runs a forward pass using the MaxText implementation and compares the resulting logits against the golden
# ones, asserting that they match within a tolerance.
#
# Utilities and Colab notebooks used to generate the golden logits are available in MaxText/scratch_code â€” for example:
#   MaxText/scratch_code/golden_llama2-7b_export.ipynb

"""Check if the logits generated by a model's MaxText/HF implementation matches golden logits for the same inputs"""

import argparse
import os
import sys
import numpy as np
import jax
import jax.numpy as jnp
import jsonlines
import torch.nn.functional as F
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

from MaxText.utils.ckpt_conversion.utils.hf_utils import (
    convert_jax_weight_to_torch,
)
from MaxText import max_logging
from MaxText import maxtext_utils
from MaxText import pyconfig
from MaxText.common_types import DECODING_ACTIVE_SEQUENCE_INDICATOR
from MaxText.globals import PKG_DIR
from MaxText.layers import models
from MaxText.layers import quantizations

from MaxText.layers.llama2 import LlamaDecoderLayer

from test_weights import patch_orbax_weights, compare_hf_orbax_model_weights

def str2bool(v):
    if isinstance(v, bool):
        return v
    v = v.lower()
    if v in ("yes", "true", "t", "1"):
        return True
    elif v in ("no", "false", "f", "0"):
        return False
    else:
        raise ValueError(f"Invalid boolean value: {v}")

def to_numpy(x):
    if isinstance(x, torch.Tensor):
        return x.detach().cpu().float().numpy()
    elif isinstance(x, jax.Array):
        return np.array(x, dtype=np.float32)
    return np.array(x)


def log_diff(name, a, b):
    a_np, b_np = to_numpy(a), to_numpy(b)
    diff = np.max(np.abs(a_np - b_np))
    mean_diff = np.mean(np.abs(a_np - b_np))
    print(f"[{name:<40}] max|diff|={diff:.3e}, mean|diff|={mean_diff:.3e}")


def compare_module_outputs(hf_model, mt_model, mt_state, tokenizer, config, prompts, mesh):
    bound = mt_model.bind(mt_state.params)

    for prompt in prompts:
        print(f"\n=== Prompt: {prompt} ===")
        inputs = tokenizer(prompt, return_tensors="pt")
        input_ids = inputs["input_ids"]

        # ----- Prepare inputs for both -----
        ids = jnp.asarray(input_ids.numpy(), dtype=jnp.int32)
        positions = jnp.arange(ids.shape[1], dtype=jnp.int32)[None, :].repeat(ids.shape[0], 0)
        segs = jnp.ones_like(ids) * DECODING_ACTIVE_SEQUENCE_INDICATOR

        # HF forward with hidden states
        with torch.no_grad():
            hf_out = hf_model(
                input_ids,
                output_hidden_states=True,
                output_attentions=True,
                return_dict=True,
            )

        # MaxText embedding
        embed_mt = bound.shared_embedding(ids)
        embed_hf = hf_out.hidden_states[0]
        log_diff("Embedding", embed_mt, embed_hf)
        
        batch_size, seq_len = input_ids.shape
        position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand(batch_size, -1)
        position_embeddings = hf_model.model.rotary_emb(embed_hf, position_ids)

        # Loop over layers
        for layer_idx in range(config.num_decoder_layers):
            hf_layer = hf_model.model.layers[layer_idx]
            # mt_layer = bound.decoder.decoder_layer[layer_idx]
            params = mt_state.params['params']['decoder'][f'layers_{layer_idx}']
            
            unbound_layer = LlamaDecoderLayer(config=config, mesh=mesh)
            outputs, mutated_vars = unbound_layer.apply(
                {"params": params},
                embed_mt,
                decoder_segment_ids=segs,
                decoder_positions=positions,
                deterministic=True,
                model_mode="train",
                mutable=["intermediates"]
            )
            
            mt_norm = mutated_vars['intermediates']['lnx_rms']

            # Input norm
            with torch.no_grad():
                hf_norm = hf_layer.input_layernorm(embed_hf)
            
            log_diff(f"Layer {layer_idx} pre-attn norm", mt_norm, hf_norm)

            # Self-attention output
            with torch.no_grad():
                hf_attn_out = hf_layer.self_attn(
                    hf_norm, 
                    position_embeddings=position_embeddings, 
                    attention_mask=None, 
                    past_key_value=None, 
                    output_attentions=False
                )[0]
            mt_attn_out = mutated_vars['intermediates']['attention_lnx']
            log_diff(f"Layer {layer_idx} self-attn", mt_attn_out, hf_attn_out)

            # Post-attn norm
            with torch.no_grad():
                hf_post_norm = hf_layer.post_attention_layernorm(hf_norm + hf_attn_out)
            mt_post_norm = mutated_vars['intermediates']['hidden_states']
            log_diff(f"Layer {layer_idx} post-attn norm", mt_post_norm, hf_post_norm)

            # MLP
            with torch.no_grad():
                hf_mlp_out = hf_layer.mlp(hf_post_norm)
            mt_mlp_out = mutated_vars['intermediates']['mlp_lnx']
            log_diff(f"Layer {layer_idx} mlp", mt_mlp_out, hf_mlp_out)

            # Residual output
            embed_hf = hf_post_norm + hf_mlp_out
            embed_mt = mt_post_norm[0] + mt_mlp_out[0]
            # import pdb; pdb.set_trace()

        # Final norm
        with torch.no_grad():
            hf_final_norm = hf_model.model.norm(embed_hf)
        mt_final_norm = norm_layer.apply({"params": mt_state.params["decoder"]["norm_layer"]}, embed_mt)
        # mt_final_norm = bound.decoder.norm_layer(num_features=embed_mt.shape[-1])(embed_mt)
        log_diff("Final norm", mt_final_norm, hf_final_norm)

        # Logits
        with torch.no_grad():
            hf_logits = hf_model.lm_head(hf_final_norm)
        mt_logits = bound.decoder._apply_output_head(mt_final_norm, deterministic=True, model_mode="train")
        log_diff("Logits", mt_logits, hf_logits)

def main(config, test_args):  # pylint: disable=W0621
    """Test the Whole Model of model_name"""
    """Comparing maxtext model with HF model on-the-fly"""
    if test_args.hf_model_path == "":
        raise ValueError
    hf_model = AutoModelForCausalLM.from_pretrained(test_args.hf_model_path, torch_dtype=torch.bfloat16)
    tokenizer = AutoTokenizer.from_pretrained(test_args.hf_model_path)
    tokenizer.pad_token = tokenizer.eos_token

    init_rng = jax.random.PRNGKey(config.init_weights_seed)
    init_rng, rng1 = jax.random.split(init_rng)
    devices_array = maxtext_utils.create_device_mesh(config)
    mesh = jax.sharding.Mesh(devices_array, config.mesh_axes)
    quant = quantizations.configure_quantization(config)
    maxtext_model = models.Transformer(config, mesh, quant=quant)
    maxtext_state, _ = maxtext_utils.setup_decode_state(maxtext_model, config, rng1, mesh, None)

    prompts = ["I love to", "Today is a", "What is the"]

    compare_module_outputs(hf_model, maxtext_model, maxtext_state, tokenizer, config, prompts, mesh)

if __name__ == "__main__":
  jax.config.update("jax_default_prng_impl", "unsafe_rbg")
  os.environ["TF_CPP_MIN_LOG_LEVEL"] = "0"

  parser = argparse.ArgumentParser()
  parser.add_argument("--atol", type=float, required=False, default=0.1)
  parser.add_argument("--rtol", type=float, required=False, default=0.1)
  parser.add_argument("--token_size", type=int, required=False)
  parser.add_argument("--max_kl_div", type=float, required=False, default=0.1)
  parser.add_argument("--golden_logits_path", type=str, required=False, default="")
  parser.add_argument("--hf_model_path", type=str, required=False, default="")
  parser.add_argument("--run_hf_model", type=str2bool, required=False, default=False)
  test_args, _ = parser.parse_known_args()

  # Remove args defined in this test file to avoid error from pyconfig
  model_args = sys.argv
  to_remove_args = [
      "--atol",
      "--rtol",
      "--token_size",
      "--max_kl_div",
      "--golden_logits_path",
      "--hf_model_path",
      "--run_hf_model",
  ]
  for arg in to_remove_args:
    model_args = [s for s in model_args if not s.startswith(arg)]

  cfg = pyconfig.initialize(model_args)
  main(cfg, test_args)